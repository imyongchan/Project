import time
from .fetch import fetch_html
from .parse import parse_list_page, parse_detail_page
from .save import save_news
import traceback

def crawl_news():
    """
    ë‰´ìŠ¤ ì „ì²´ í¬ë¡¤ë§ (1~5í˜ì´ì§€)(ì„ì‹œ)
    fetch â†’ parse â†’ detail fetch â†’ detail parse â†’ save
    """
    print(f"ğŸ§¡ í¬ë¡¤ë§ ì‹œì‘")
    for page in range(1, 2):

        try:
            list_url = f"http://sanjaenews.co.kr/news/list.php?&mcode=m641vf2&vg=&page={page}"

            # 1) ëª©ë¡ HTML ìˆ˜ì§‘
            list_soup = fetch_html(list_url)

            # 2) ëª©ë¡ íŒŒì‹±
            articles = parse_list_page(list_soup) 
            if not articles:    # articles = ê° ê¸°ì‚¬ ì •ë³´(dict) ê°€ ë‹´ê¸´ list
                print("ë” ì´ìƒ í•­ëª© ì—†ìŒ. ì¢…ë£Œ.")
                break

        except Exception as e:
            print("âŒ ëª©ë¡ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨:", e)
            traceback.print_exc()
            continue

        # ìƒì„¸í˜ì´ì§€ ì²˜ë¦¬
        for art in articles:
            try:
                detail_soup = fetch_html(art["link"])
                detail = parse_detail_page(detail_soup)
                art["writer"] = detail["writer"]  # writer í‚¤ ê°’ ìƒˆë¡œ ì¶”ê°€

                save_news(art) # DB ì €ì¥

            except Exception as e:
                print(f"âŒ ìƒì„¸í˜ì´ì§€ ì‹¤íŒ¨: {art.get('link')}", e)
                continue

            time.sleep(0.15)

    print("ğŸŒ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ")
    
    from datetime import datetime
    end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"ğŸ•’ í¬ë¡¤ë§ ì¢…ë£Œ ì‹œê°„: {end_time}")
