import time
from .fetch import fetch_html
from .parse import parse_list_page, parse_detail_page
from .save import save_news

def crawl_news():
    """
    λ‰΄μ¤ μ „μ²΄ ν¬λ΅¤λ§
    """
    print(f"\n===== π  λ‰΄μ¤ ν¬λ΅¤λ§ μ‹μ‘ π  =====")

    page = 1

    while True:
        print(f"\nβ–¶ λ©λ΅ νμ΄μ§€ {page} μμ§‘ μ¤‘...")

        try:
            list_url = f"http://sanjaenews.co.kr/news/list.php?&mcode=m641vf2&vg=&page={page}"

            # 1) λ©λ΅ HTML μμ§‘
            list_soup = fetch_html(list_url)

            # 2) λ©λ΅ νμ‹±
            articles = parse_list_page(list_soup)

            # μΆ…λ£ μ΅°κ±΄
            if not articles:
                print("π λ” μ΄μƒ κΈ°μ‚¬ μ—†μ β†’ ν¬λ΅¤λ§ μΆ…λ£")
                break

        except Exception as e:
            print("β λ©λ΅ νμ΄μ§€ μμ§‘ μ‹¤ν¨:", e)
            break   

        # μƒμ„Ένμ΄μ§€ μ²λ¦¬
        for art in articles:
            try:
                detail_soup = fetch_html(art["link"])
                detail = parse_detail_page(detail_soup)

                art["writer"] = detail.get("writer")
                save_news(art)

            except Exception as e:
                print(f"β μƒμ„Ένμ΄μ§€ μ‹¤ν¨: {art.get('link')}", e)
                continue

            time.sleep(0.2)  

        page += 1
        time.sleep(0.5)      # β­ νμ΄μ§€ λ‹¨μ„ ν΄μ‹

    from datetime import datetime
    end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"\n====== λ‰΄μ¤ μ „μ²΄ ν¬λ΅¤λ§ μ™„λ£ =====")
    print(f"π•’ μΆ…λ£ μ‹κ°„: {end_time}")