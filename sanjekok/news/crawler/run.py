import time
from .fetch import fetch_html
from .parse import parse_list_page, parse_detail_page
from .save import save_news
import traceback

def crawl_news():
    """
    ë‰´ìŠ¤ ì „ì²´ í¬ë¡¤ë§
    """
    print(f"\n===== ğŸŸ  ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ ğŸŸ  =====")

    page = 1

    while True:
        print(f"\nâ–¶ ëª©ë¡ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")

        try:
            list_url = f"http://sanjaenews.co.kr/news/list.php?&mcode=m641vf2&vg=&page={page}"

            # 1) ëª©ë¡ HTML ìˆ˜ì§‘
            list_soup = fetch_html(list_url)

            # 2) ëª©ë¡ íŒŒì‹±
            articles = parse_list_page(list_soup)

            # ì¢…ë£Œ ì¡°ê±´
            if not articles:
                print("ğŸŒ ë” ì´ìƒ ê¸°ì‚¬ ì—†ìŒ â†’ í¬ë¡¤ë§ ì¢…ë£Œ")
                break

        except Exception as e:
            print("âŒ ëª©ë¡ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨:", e)
            traceback.print_exc()
            break   # ë‰´ìŠ¤ëŠ” ì—¬ê¸°ì„œ ëŠëŠ” ê²Œ ì•ˆì „

        # ìƒì„¸í˜ì´ì§€ ì²˜ë¦¬
        for art in articles:
            try:
                detail_soup = fetch_html(art["link"])
                detail = parse_detail_page(detail_soup)

                art["writer"] = detail.get("writer")
                save_news(art)

            except Exception as e:
                print(f"âŒ ìƒì„¸í˜ì´ì§€ ì‹¤íŒ¨: {art.get('link')}", e)
                continue

            time.sleep(0.2)  

        page += 1
        time.sleep(0.5)      # â­ í˜ì´ì§€ ë‹¨ìœ„ íœ´ì‹

    from datetime import datetime
    end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"\n====== ë‰´ìŠ¤ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ =====")
    print(f"ğŸ•’ ì¢…ë£Œ ì‹œê°„: {end_time}")