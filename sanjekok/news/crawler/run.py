# news/crawler/run.py

import time
import traceback
from .fetch import fetch_html
from .parse import parse_list_page, parse_detail_page
from .save import save_news

def crawl_news():
    """
    λ‰΄μ¤ μ „μ²΄ ν¬λ΅¤λ§ (1~5νμ΄μ§€)
    fetch β†’ parse β†’ detail fetch β†’ detail parse β†’ save
    """

    for page in range(1, 6):
        print(f"π“„ {page} νμ΄μ§€ μμ§‘ μ¤‘...")

        try:
            list_url = (
                "http://sanjaenews.co.kr/news/list.php?"
                "&mcode=m641vf2&vg=photo&page=" + str(page)
            )

            # 1) λ©λ΅ HTML μμ§‘
            list_soup = fetch_html(list_url)

            # 2) λ©λ΅ νμ‹±
            articles = parse_list_page(list_soup)
            if not articles:
                print("  π‘‰ λ” μ΄μƒ ν•­λ© μ—†μ. μΆ…λ£.")
                break

        except Exception as e:
            print("β λ©λ΅ νμ΄μ§€ μμ§‘ μ‹¤ν¨:", e)
            traceback.print_exc()
            continue

        # μƒμ„Ένμ΄μ§€ μ²λ¦¬
        for art in articles:
            try:
                detail_soup = fetch_html(art["link"])
                detail = parse_detail_page(detail_soup)
                art["writer"] = detail["writer"]

                save_news(art)

            except Exception as e:
                print(f"β μƒμ„Ένμ΄μ§€ μ‹¤ν¨: {art.get('link')}")
                traceback.print_exc()
                continue

            time.sleep(0.15)

    print("μ „μ²΄ ν¬λ΅¤λ§ μ™„λ£")
