# safe/crawler/run.py

from .fetch import fetch_page
from .parse import parse_list
from .save import save_items
from datetime import datetime

TYPE_CODES = [
    "12",  # OPS
    "02",  # ë™ì˜ìƒ
    "01",  # ì±…ì
    "07",  # PPT
    ""     # ê¸°íƒ€
]


def crawl_safe():
    print("\n=============== ì•ˆì „ìë£Œ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ ===============\n")

    for shpCd in TYPE_CODES:
        print(f"\n===== ğŸŸ  ìë£Œí˜•íƒœ [{shpCd or 'ê¸°íƒ€'}] í¬ë¡¤ë§ ì‹œì‘ ğŸŸ  =====")

        try:
            for page in range(1, 3):  # í˜ì´ì§€ 1~2ê¹Œì§€ í…ŒìŠ¤íŠ¸(ì„ì‹œ)
                print(f" í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")

                # 1) API ìš”ì²­
                try:
                    data = fetch_page(shpCd=shpCd, page=page)
                except Exception as e:
                    print(f" âŒ fetch ì‹¤íŒ¨: {e}")
                    break

                # 2) íŒŒì‹±
                items = parse_list(data, shpCd)

                # 3) í˜ì´ì§€ ì¢…ë£Œ ê°ì§€
                if not items:
                    print(" ë” ì´ìƒ ë°ì´í„° ì—†ìŒ â†’ ë‹¤ìŒ ìë£Œí˜•íƒœë¡œ ì´ë™")
                    break

                # 4) ì €ì¥
                save_items(items)

        except Exception as e:
            print(f" âŒ [{shpCd or 'ê¸°íƒ€'}] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

        print(f"===== ğŸŒ ìë£Œí˜•íƒœ [{shpCd or 'ê¸°íƒ€'}] ì™„ë£Œ =====")



    print("\n======= ì•ˆì „ìë£Œ ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ =======")
    end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"ğŸ•’ í¬ë¡¤ë§ ì¢…ë£Œ ì‹œê°„: {end_time}\n")
