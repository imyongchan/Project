# safe/crawler/run.py
import time
from .fetch import fetch_page
from .parse import parse_list
from .save import save_items
from datetime import datetime

TYPE_CODES = [
    "12",  # OPS
    "02",  # ë™ì˜ìƒ
    "01",  # ì±…ì
    "07",  # PPT
    ""     # ê¸°íƒ€
]


def crawl_safe():
    print("\n===== ì•ˆì „ìë£Œ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ =====\n")

    for shpCd in TYPE_CODES:
        print(f"\n===== ğŸŸ  ìë£Œí˜•íƒœ [{shpCd or 'ê¸°íƒ€'}] ì‹œì‘ ğŸŸ  =====")

        page = 1

        while True:
            print(f" â–¶ í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")

            try:
                data = fetch_page(shpCd=shpCd, page=page)
            except Exception as e:
                print(f" âŒ fetch ì‹¤íŒ¨: {e}")
                break

            items = parse_list(data, shpCd)

            # ì¢…ë£Œ ì¡°ê±´
            if not items:
                print(" ğŸŒ ë” ì´ìƒ ë°ì´í„° ì—†ìŒ â†’ ë‹¤ìŒ ìë£Œí˜•íƒœë¡œ ì´ë™")
                break

            save_items(items)

            page += 1
            time.sleep(0.3)  # â­ API ë°°ë ¤

        print(f"===== ğŸŒ ìë£Œí˜•íƒœ [{shpCd or 'ê¸°íƒ€'}] ì™„ë£Œ =====")

    from datetime import datetime
    end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print("\n===== ì•ˆì „ìë£Œ ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ =====")
    print(f"ğŸ•’ ì¢…ë£Œ ì‹œê°„: {end_time}\n")
